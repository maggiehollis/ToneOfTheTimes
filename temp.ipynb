{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b85f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Looking for .env file and loading it\n",
    "load_dotenv() \n",
    "\n",
    "nyt_api = os.getenv(\"NYT_ID\")\n",
    "guardian_api =  os.getenv(\"GUARDIAN_ID\")\n",
    "\n",
    "# Get the id and secret from the .env\n",
    "spotify_client_id = os.getenv(\"SPOTIFY_CLIENT_ID\" )\n",
    "spotify_client_secret = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "genius_token = os.getenv(\"GENIUS_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc727e3",
   "metadata": {},
   "source": [
    "# News Collection\n",
    "Pull today's headline from the NYT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d5b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f315a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = f'https://api.nytimes.com/svc/topstories/v2/home.json?api-key={nyt_api}'\n",
    "\n",
    "response = requests.get(URL)\n",
    "\n",
    "nyt_articles = []\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        # Assume the first article is the main front-page article\n",
    "        for article in data['results']:\n",
    "            nyt_articles.append(article['title'] + \": \" + article['title'])\n",
    "    else:\n",
    "        print(\"No articles found.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "    \n",
    "print(len(nyt_articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "811fa6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Parameters\n",
    "SECTION = 'us-news'\n",
    "DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "URL = 'https://content.guardianapis.com/search'\n",
    "NUM_ARTICLES = 8\n",
    "\n",
    "params = {\n",
    "    'section': SECTION,\n",
    "    'from-date': DATE,\n",
    "    'to-date': DATE,\n",
    "    'order-by': 'newest',\n",
    "    'page-size': NUM_ARTICLES,\n",
    "    'show-fields': 'trailText',\n",
    "    'api-key': guardian_api\n",
    "}\n",
    "\n",
    "guardian_articles = []\n",
    "\n",
    "try:\n",
    "    response = requests.get(URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get('response', {}).get('status') == 'ok' and data['response']['results']:\n",
    "        articles = data['response']['results']\n",
    "        with open('guardian_us_headlines.txt', 'w', encoding='utf-8') as file:\n",
    "            for idx, article in enumerate(articles, start=1):\n",
    "                title = article.get('webTitle', 'No Title')\n",
    "                abstract = article.get('fields', {}).get('trailText', 'No Abstract')\n",
    "                guardian_articles.append(title + \": \" + abstract)\n",
    "    else:\n",
    "        print(\"No articles found for today.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(len(guardian_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056ed26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# NLP Analysis\n",
    "Calculate sentiment of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f642e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "05b49a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYT Headline Sentiment: 0.4588\n",
      "Guardian Headline Sentiment: 0.6249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/maggiehollis/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# NYT\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenized = word_tokenize(times)\n",
    "text = [word.lower() for word in tokenized if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "news_sentiment_nyt = sentiment_analyzer.polarity_scores(' '.join(tokenized))\n",
    "\n",
    "print(\"NYT Headline Sentiment:\", news_sentiment_nyt['compound'])\n",
    "\n",
    "# Guardian\n",
    "tokenized = word_tokenize(guardian)\n",
    "text = [word.lower() for word in tokenized if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "news_sentiment_g = sentiment_analyzer.polarity_scores(' '.join(tokenized))\n",
    "\n",
    "print(\"Guardian Headline Sentiment:\", news_sentiment_g['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12906a5",
   "metadata": {},
   "source": [
    "# Song Recs\n",
    "Search for songs in the spotify API that have audio features we matched with the mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "031501a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spotipy\n",
      "  Downloading spotipy-2.25.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (4.12.3)\n",
      "Collecting redis>=3.5.3 (from spotipy)\n",
      "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from spotipy) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from spotipy) (2.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (2024.12.14)\n",
      "Downloading spotipy-2.25.1-py3-none-any.whl (31 kB)\n",
      "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
      "Installing collected packages: redis, spotipy\n",
      "Successfully installed redis-5.2.1 spotipy-2.25.1\n"
     ]
    }
   ],
   "source": [
    "! pip install spotipy beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "236d516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "from requests import post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e17c3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(client_id, client_secret):\n",
    "    # Encode the client ID and client secret\n",
    "    auth_string = client_id + \":\" + client_secret\n",
    "    auth_bytes = auth_string.encode(\"utf-8\")\n",
    "    auth_base64 = base64.b64encode(auth_bytes).decode(\"utf-8\")\n",
    "    \n",
    "    # Define the URL and headers for the POST request\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Basic {auth_base64}\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    \n",
    "    # Set the data payload for the POST request\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    \n",
    "    # Make the POST request to get the token\n",
    "    result = post(url, headers=headers, data=data)\n",
    "    token = result.json()\n",
    "    # notice how we JSONify the Response object\n",
    "\n",
    "    return token # Return the access token from the JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the token\n",
    "token = get_token(spotify_client_id, spotify_client_secret)\n",
    "access_token = token[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8786e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Kendrick Lamar - luther (with sza)\n",
      "\n",
      "Processing: Drake - NOKIA\n",
      "\n",
      "Processing: Lady Gaga - Die With A Smile\n",
      "\n",
      "Processing: BigXthaPlug - All The Way (feat. Bailey Zimmerman)\n",
      "\n",
      "Processing: Chappell Roan - Pink Pony Club\n",
      "\n",
      "Processing: Shaboozey - A Bar Song (Tipsy)\n",
      "\n",
      "Processing: Alex Warren - Ordinary\n",
      "\n",
      "Processing: Teddy Swims - Lose Control\n",
      "\n",
      "Processing: Morgan Wallen - I'm The Problem\n",
      "\n",
      "Processing: Benson Boone - Beautiful Things\n",
      "\n",
      "Processing: Billie Eilish - BIRDS OF A FEATHER\n",
      "\n",
      "Processing: Morgan Wallen - Just In Case\n",
      "\n",
      "Processing: ROSÉ - APT.\n",
      "\n",
      "Processing: Doechii - Anxiety\n",
      "\n",
      "Processing: Gracie Abrams - That’s So True\n",
      "\n",
      "Processing: Kendrick Lamar - Not Like Us\n",
      "\n",
      "Processing: Sabrina Carpenter - Espresso\n",
      "\n",
      "Processing: Kendrick Lamar - tv off (feat. lefty gunplay)\n",
      "\n",
      "Processing: Morgan Wallen - Love Somebody\n",
      "\n",
      "Processing: Post Malone - I Had Some Help (Feat. Morgan Wallen)\n",
      "\n",
      "Processing: Leon Thomas - MUTT\n",
      "\n",
      "Processing: Lola Young - Messy\n",
      "\n",
      "Processing: Kendrick Lamar - squabble up\n",
      "\n",
      "Processing: Tate McRae - Sports car\n",
      "\n",
      "Processing: SZA - 30 For 30 (with Kendrick Lamar)\n",
      "\n",
      "Processing: The Weeknd - Timeless (feat. Playboi Carti)\n",
      "\n",
      "Processing: Sabrina Carpenter - Taste\n",
      "\n",
      "Processing: Ed Sheeran - Azizam\n",
      "\n",
      "Processing: Zach Top - I Never Lie\n",
      "\n",
      "Processing: Myles Smith - Stargazing\n",
      "\n",
      "Processing: The Marías - No One Noticed\n",
      "\n",
      "Processing: Morgan Wallen - I'm A Little Crazy\n",
      "\n",
      "Processing: Lil Tecca - Dark Thoughts\n",
      "\n",
      "Processing: Sleep Token - Caramel\n",
      "\n",
      "Processing: Doechii - DENIAL IS A RIVER\n",
      "\n",
      "Processing: Gigi Perez - Sailor Song\n",
      "\n",
      "Processing: Teddy Swims - Bad Dreams\n",
      "\n",
      "Processing: Lady Gaga - Abracadabra\n",
      "\n",
      "Processing: Chappell Roan - Good Luck, Babe!\n",
      "\n",
      "Processing: Chappell Roan - The Giver\n",
      "\n",
      "Processing: Ariana Grande - twilight zone\n",
      "\n",
      "Processing: Riley Green - Worst Way\n",
      "\n",
      "Processing: Sam Barber - Indigo (feat. Avery Anna)\n",
      "\n",
      "Processing: Shaboozey - Good News\n",
      "\n",
      "Processing: Billie Eilish - WILDFLOWER\n",
      "\n",
      "Processing: Playboi Carti - RATHER LIE (with The Weeknd)\n",
      "\n",
      "Processing: Benson Boone - Sorry I'm Here For Someone Else\n",
      "\n",
      "Processing: Bad Bunny - DtMF\n",
      "\n",
      "Processing: The Weeknd - Cry For Me\n",
      "\n",
      "Processing: Chris Brown - Residuals\n",
      "\n",
      "Processing: SZA - BMF\n",
      "\n",
      "Processing: Miley Cyrus - End of the World\n",
      "\n",
      "Processing: Fuerza Regida - ME JALO\n",
      "\n",
      "Processing: Bad Bunny - BAILE INoLVIDABLE\n",
      "\n",
      "Processing: Megan Moroney - Am I Okay?\n",
      "\n",
      "Processing: PARTYNEXTDOOR - SOMEBODY LOVES ME\n",
      "\n",
      "Processing: Tate McRae - Revolving door\n",
      "\n",
      "Processing: Selena Gomez - Ojos Tristes (with The Marías)\n",
      "\n",
      "Processing: Morgan Wallen - Smile\n",
      "\n",
      "Processing: Ariana Grande - dandelion\n",
      "\n",
      "Processing: Alemán - Te Quería Ver\n",
      "\n",
      "Processing: Ella Langley - weren't for the wind\n",
      "\n",
      "Processing: Playboi Carti - EVIL J0RDAN\n",
      "\n",
      "Processing: Selena Gomez - Call Me When You Break Up (with Gracie Abrams)\n",
      "\n",
      "Processing: Neton Vega - Loco\n",
      "\n",
      "Processing: Drake - GIMME A HUG\n",
      "\n",
      "Processing: Kendrick Lamar - peekaboo (feat. azchike)\n",
      "\n",
      "Processing: Malcolm Todd - Chest Pain (I Love)\n",
      "\n",
      "Processing: Brandon Lake - Hard Fought Hallelujah\n",
      "\n",
      "Processing: sombr - back to friends\n",
      "\n",
      "Processing: PARTYNEXTDOOR - DIE TRYING\n",
      "\n",
      "Processing: Sabrina Carpenter - Busy Woman\n",
      "\n",
      "Processing: Summer Walker - Heart Of A Woman\n",
      "\n",
      "Processing: Thomas Rhett - Somethin’ ‘Bout A Woman (feat. Teddy Swims)\n",
      "\n",
      "Processing: Bad Bunny - EoO\n",
      "\n",
      "Processing: Tyler, The Creator - Like Him (feat. Lola Young)\n",
      "\n",
      "Processing: Don Toliver - No Pole\n",
      "\n",
      "Processing: John Morgan - Friends Like That (feat. Jason Aldean)\n",
      "\n",
      "Processing: Ravyn Lenae - Love Me Not\n",
      "\n",
      "Processing: Bailey Zimmerman - Holy Smokes\n",
      "\n",
      "Processing: Blake Shelton - Texas\n",
      "\n",
      "Processing: Teddy Swims - Are You Even Real (feat. Givēon)\n",
      "\n",
      "Processing: Neton Vega - Morena\n",
      "\n",
      "Processing: d4vd - Feel It (From “Invincible”)\n",
      "\n",
      "Processing: Rauw Alejandro - Qué Pasaría...\n",
      "\n",
      "Processing: The Marías - Back To Me\n",
      "\n",
      "Processing: Nate Smith - Fix What You Didn't Break\n",
      "\n",
      "Processing: Sleep Token - Emergence\n",
      "\n",
      "Processing: Ariana Grande - intro (end of the world) - extended\n",
      "\n",
      "Processing: Bad Bunny - NUEVAYoL\n",
      "\n",
      "Processing: LOCASH - Hometown Home\n",
      "\n",
      "Processing: BigXthaPlug - The Largest\n",
      "\n",
      "Processing: Imogen Heap - Headlock\n",
      "\n",
      "Processing: Tito Double P - TATTOO\n",
      "\n",
      "Processing: Kane Brown - Backseat Driver\n",
      "\n",
      "Processing: sombr - undressed\n",
      "\n",
      "Processing: Ella Mai - Little Things\n",
      "\n",
      "Processing: Fuerza Regida - Por Esos Ojos\n",
      "\n",
      "Processing: Kane Brown - Haunted\n",
      "\n",
      "Processing: Rauw Alejandro - Khé?\n"
     ]
    }
   ],
   "source": [
    "def get_usa_top_50(token):\n",
    "    playlist_id = '6UeSakyzhiEt4NB3UAd6NQ'  # USA Top 50 Playlist ID\n",
    "    url = f'https://api.spotify.com/v1/playlists/{playlist_id}'\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    playlist_data = response.json()\n",
    "\n",
    "    top_50_tracks = []\n",
    "\n",
    "    for item in playlist_data['tracks']['items']:\n",
    "        track = item['track']\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        top_50_tracks.append({'track': track_name, 'artist': artist_name})\n",
    "\n",
    "    return top_50_tracks\n",
    "\n",
    "\n",
    "# --- Genius API Part ---\n",
    "def get_genius_url(genius_token, artist, title):\n",
    "    search_url = 'https://api.genius.com/search'\n",
    "    headers = {'Authorization': f'Bearer {genius_token}'}\n",
    "    query = f\"{artist} {title}\"\n",
    "    params = {'q': query}\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    hits = data['response']['hits']\n",
    "    if hits:\n",
    "        return hits[0]['result']['url']\n",
    "    return None\n",
    "\n",
    "def get_lyrics(genius_url):\n",
    "    page = requests.get(genius_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics_containers = soup.find_all('div', attrs={\"data-lyrics-container\": \"true\"})\n",
    "    lyrics = \"\\n\".join([div.get_text(strip=True) for div in lyrics_containers])\n",
    "    return lyrics\n",
    "\n",
    "usa_top_50 = get_usa_top_50(access_token)\n",
    "sentiments = {}\n",
    "\n",
    "for song in usa_top_50:\n",
    "    artist = song['artist']\n",
    "    title = song['track']\n",
    "    print(f\"\\nProcessing: {artist} - {title}\")\n",
    "    try:\n",
    "        genius_url = get_genius_url(genius_token, artist, title)\n",
    "        if genius_url:\n",
    "            lyrics = get_lyrics(genius_url)\n",
    "            sentiment = sia.polarity_scores(lyrics)\n",
    "            sentiments[f\"{artist} - {title}\"] = sentiment\n",
    "        else:\n",
    "            print(\"Genius URL not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {artist} - {title}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8e344168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYT: We recommend you listen to: That’s So True by Gracie Abrams\n",
      "Guardian: We recommend you listen to: Hometown Home by LOCASH\n"
     ]
    }
   ],
   "source": [
    "goal = news_sentiment_nyt['compound']\n",
    "distance = 100\n",
    "closest_song = \"None\"\n",
    "closest_artist = \"None\"\n",
    "\n",
    "for song, sentiment in sentiments.items():\n",
    "    if abs(sentiment['compound'] - goal) < distance:\n",
    "        distance = abs(sentiment['compound'] - goal)\n",
    "        closest_song = song.split(\" - \")[1]\n",
    "        closest_artist = song.split(\" - \")[0]\n",
    "        \n",
    "print(f\"NYT: We recommend you listen to: {closest_song} by {closest_artist}\")\n",
    "\n",
    "goal = news_sentiment_g['compound']\n",
    "distance = 100\n",
    "closest_song = \"None\"\n",
    "closest_artist = \"None\"\n",
    "\n",
    "for song, sentiment in sentiments.items():\n",
    "    if abs(sentiment['compound'] - goal) < distance:\n",
    "        distance = abs(sentiment['compound'] - goal)\n",
    "        closest_song = song.split(\" - \")[1]\n",
    "        closest_artist = song.split(\" - \")[0]\n",
    "        \n",
    "print(f\"Guardian: We recommend you listen to: {closest_song} by {closest_artist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc567e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
