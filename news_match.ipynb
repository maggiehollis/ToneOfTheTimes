{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Looking for .env file and loading it\n",
    "load_dotenv() \n",
    "\n",
    "nyt_api = os.getenv(\"NYT_ID\")\n",
    "guardian_api =  os.getenv(\"GUARDIAN_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc727e3",
   "metadata": {},
   "source": [
    "# News Collection\n",
    "Pull today's headline from the NYT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f315a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = f'https://api.nytimes.com/svc/topstories/v2/home.json?api-key={nyt_api}'\n",
    "\n",
    "response = requests.get(URL)\n",
    "\n",
    "nyt_articles = []\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        # Assume the first article is the main front-page article\n",
    "        for article in data['results']:\n",
    "            nyt_articles.append(article['title'] + \": \" + article['abstract'])\n",
    "    else:\n",
    "        print(\"No articles found.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "    \n",
    "# curr_article = nyt_articles[0]\n",
    "# print(\"Top NYT article:\", curr_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "811fa6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Guardian article: Woman in Florida deported to Cuba says she was forced to leave baby daughter: Heidy Sánchez says she was told her 17-month-old, who has health problems and is breastfeeding, couldn’t go with her\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Parameters\n",
    "SECTION = 'us-news'\n",
    "DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "URL = 'https://content.guardianapis.com/search'\n",
    "NUM_ARTICLES = 8\n",
    "\n",
    "params = {\n",
    "    'section': SECTION,\n",
    "    'from-date': DATE,\n",
    "    'to-date': DATE,\n",
    "    'order-by': 'newest',\n",
    "    'page-size': NUM_ARTICLES,\n",
    "    'show-fields': 'trailText',\n",
    "    'api-key': guardian_api\n",
    "}\n",
    "\n",
    "guardian_articles = []\n",
    "\n",
    "try:\n",
    "    response = requests.get(URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get('response', {}).get('status') == 'ok' and data['response']['results']:\n",
    "        articles = data['response']['results']\n",
    "        for idx, article in enumerate(articles, start=1):\n",
    "            title = article.get('webTitle', 'No Title')\n",
    "            abstract = article.get('fields', {}).get('trailText', 'No Abstract')\n",
    "            guardian_articles.append(title + \": \" + abstract)\n",
    "    else:\n",
    "        print(\"No articles found for today.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "curr_article = guardian_articles[4]\n",
    "print(\"Top Guardian article:\", curr_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056ed26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# NLP Analysis of News\n",
    "Calculate sentiment of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedbefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigdata2025/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def get_emotions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = F.softmax(logits, dim=1)[0]\n",
    "    labels = model.config.id2label\n",
    "    return {labels[i]: float(probs[i]) for i in range(len(probs))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05b49a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/maggiehollis/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maggiehollis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maggiehollis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYT Headline Sentiment: {'neg': 0.33, 'neu': 0.67, 'pos': 0.0, 'compound': -0.7096}\n",
      "NYT Headline Emotions: {'admiration': 9.457051783101633e-05, 'amusement': 0.000182974457857199, 'anger': 0.0004586175491567701, 'annoyance': 0.0014701030449941754, 'approval': 0.0009689170401543379, 'caring': 0.0002534352825023234, 'confusion': 0.00021465042664203793, 'curiosity': 0.00014806790568400174, 'desire': 0.0002997353149112314, 'disappointment': 0.0014162200968712568, 'disapproval': 0.0007111019222065806, 'disgust': 0.0006654822500422597, 'embarrassment': 0.00019676871306728572, 'excitement': 5.028406667406671e-05, 'fear': 0.00033870089100673795, 'gratitude': 5.10587306052912e-05, 'grief': 0.0002485248551238328, 'joy': 8.179308497346938e-05, 'love': 0.00011305588122922927, 'nervousness': 0.00010256931273033842, 'optimism': 0.00019025354413315654, 'pride': 2.6401559807709418e-05, 'realization': 0.0014604241587221622, 'relief': 6.225807737791911e-05, 'remorse': 0.0001954384642885998, 'sadness': 0.004896188620477915, 'surprise': 9.034254617290571e-05, 'neutral': 0.9850120544433594}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Setup NLTK\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Disable TensorFlow in HuggingFace Transformers (optional for PyTorch-only)\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "# Stopwords and Sentiment\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# --- NYT Headline ---\n",
    "tokenized_nyt = word_tokenize(curr_article)\n",
    "filtered_nyt = [word.lower() for word in tokenized_nyt if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "nyt_text = ' '.join(filtered_nyt)\n",
    "\n",
    "# VADER Sentiment\n",
    "news_sentiment_nyt = sentiment_analyzer.polarity_scores(nyt_text)\n",
    "print(\"NYT Headline Sentiment:\", news_sentiment_nyt)\n",
    "\n",
    "# Emotion Detection\n",
    "emotions_nyt = get_emotions(curr_article)  # Truncate if needed\n",
    "print(\"NYT Headline Emotions:\", emotions_nyt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ccfd61",
   "metadata": {},
   "source": [
    "# Song Recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e344168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Matching Song:\n",
      "Artist: Peter Kruder\n",
      "Title: High Noon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load csv\n",
    "df = pd.read_csv(\"song_sentiments_emotions.csv\")\n",
    "df.columns = [col.strip().lower() for col in df.columns]\n",
    "df_full = df.copy()\n",
    "\n",
    "# Remove non-numeric columns\n",
    "df_vectors = df.iloc[:, 2:]\n",
    "df_matrix = df_vectors.apply(pd.to_numeric, errors='coerce').fillna(0.0).to_numpy()\n",
    "\n",
    "# Define keys for emotions and sentiments\n",
    "emotion_keys = [\n",
    "    'admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity',\n",
    "    'desire','disappointment','disapproval','disgust','embarrassment','excitement','fear',\n",
    "    'gratitude','grief','joy','love','nervousness','neutral','optimism','pride','realization',\n",
    "    'relief','remorse','sadness','surprise'\n",
    "]\n",
    "\n",
    "sentiment_keys = ['negative', 'neutral', 'positive', 'compound']\n",
    "\n",
    "vader_to_csv = {\n",
    "    'negative': news_sentiment_nyt['neg'],\n",
    "    'neutral': news_sentiment_nyt['neu'],\n",
    "    'positive': news_sentiment_nyt['pos'],\n",
    "    'compound': news_sentiment_nyt['compound']\n",
    "}\n",
    "\n",
    "# Create input vector\n",
    "input_vector = [vader_to_csv[key] for key in sentiment_keys] + [\n",
    "    emotions_nyt.get(key, 0.0) for key in emotion_keys\n",
    "]\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity([input_vector], df_matrix)\n",
    "best_index = np.argmax(similarities)\n",
    "best_match = df_full.iloc[best_index]\n",
    "\n",
    "# Output song rec\n",
    "print(\"Best Matching Song:\")\n",
    "print(\"Artist:\", best_match['artist'])\n",
    "print(\"Title:\", best_match['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc567e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
